{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final - Aprendizagem Automatica\n",
    "## Tema Online Shoppers Intention\n",
    "### Feito por\n",
    "David Cabrita XXX\n",
    "Rendrick Carreira 201901365\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descrição dos Dados\n",
    "\n",
    "Administrative: (Inteiro) Número de páginas diferentes visitadas relacionadas com as questões administrativas do sítio Web.\n",
    "\n",
    "Administrative_Duration: (Float) Duração total, em segundos, gasta pelo usuário em páginas relacionadas com as questões administrativas do sítio Web.\n",
    "\n",
    "Informational: (Inteiro) Número de páginas diferentes visitadas relacionadas com informações sobre o sítio Web, produtos ou serviços.\n",
    "\n",
    "Informational_Duration: (Float) Duração total, em segundos, gasta pelo usuário em páginas relacionadas com informações sobre o sítio Web, produtos ou serviços.\n",
    "\n",
    "ProductRelated: (Inteiro) Número de páginas diferentes visitadas relacionadas com produtos do sítio Web.\n",
    "\n",
    "ProductRelated_Duration: (Float) Duração total, em segundos, gasta pelo usuário em páginas relacionadas com produtos do sítio Web.\n",
    "\n",
    "BounceRates: (Float) Taxa de rejeição, ou seja, a proporção de visitantes que saíram do sítio Web depois de visualizar uma única página.\n",
    "\n",
    "ExitRates: (Float) Taxa de saída, ou seja, a proporção de visitantes que saíram do sítio Web após visualizar a página em questão.\n",
    "\n",
    "PageValues: (Float) Valor médio das páginas visualizadas pelo usuário, calculado como a soma dos valores de transação dividida pelo número total de páginas visitadas.\n",
    "\n",
    "SpecialDay: (Float) Proximidade do dia da visita a um evento especial (por exemplo, feriado), sendo 0 indicativo de um dia não especial e 1 indicativo de um dia especial.\n",
    "\n",
    "Month: (Categórico) Mês da visita do usuário ao sítio Web, representado por abreviações em inglês (por exemplo, 'Jan' para janeiro, 'Feb' para fevereiro, etc.).\n",
    "\n",
    "OperatingSystems: (Inteiro) Sistema operacional utilizado pelo usuário durante a visita ao sítio Web.\n",
    "\n",
    "Browser: (Inteiro) Navegador utilizado pelo usuário durante a visita ao sítio Web.\n",
    "\n",
    "Region: (Inteiro) Região geográfica do usuário.\n",
    "\n",
    "TrafficType: (Inteiro) Tipo de tráfego que trouxe o usuário ao sítio Web.\n",
    "\n",
    "VisitorType: (Categórico) Tipo de visitante do sítio Web, podendo ser \"Returning Visitor\" (Visitante que retorna), \"New Visitor\" (Novo Visitante) ou \"Other\" (Outro).\n",
    "\n",
    "Weekend: (Booleano) Indica se a visita ocorreu em um fim de semana (TRUE) ou não (FALSE).\n",
    "\n",
    "Revenue: (Booleano) Indica se o usuário gerou receita para o sítio Web (TRUE) ou não (FALSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importações\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Definiçoes do dataframe \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# remover filtros de aviso\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação de dados\n",
    "df=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar o numero de amostras\n",
    "num_amostras = df.shape[0]\n",
    "print(\"Número de amostras:\", num_amostras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se existem algum dado que falta\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este codigo vai mostrar o tipo de dados de cada coluna\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in df.columns:\n",
    "    contagem_valores = df[coluna].value_counts()\n",
    "    print(f\"\\nContagem de valores para a coluna '{coluna}':\\n\")\n",
    "    print(contagem_valores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#administration duration\n",
    "q1=df['Administrative_Duration'].quantile(0.25)\n",
    "q3=df['Administrative_Duration'].quantile(0.75)\n",
    "iqr=q3-q1\n",
    "u_l1=q3+1.5*iqr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Administrative_Duration']=df_new['Administrative_Duration'].map(lambda x:0 if x==0 else (1 if x>0 and x<u_l1 else 2))\n",
    "df_new['Administrative_Duration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Informational_Duration']=df_new['Informational_Duration'].map(lambda x:0 if x==0 else 1)\n",
    "df_new['Informational_Duration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProductRelated_Duration\n",
    "q1=df['ProductRelated_Duration'].quantile(0.25)\n",
    "q3=df['ProductRelated_Duration'].quantile(0.75)\n",
    "iqr=q3-q1\n",
    "u_l1=q3+1.5*iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['ProductRelated_Duration']=df_new['ProductRelated_Duration'].map(lambda x:0 if x==0 else (1 if x>0 and x<u_l1 else 2))\n",
    "df_new['ProductRelated_Duration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Revenue'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administrative_Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Administrative_Duration'],df_new['Revenue'],values=df_new['BounceRates'],aggfunc='mean')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see avg.bounce rate when revenue true is very less then when it is false\n",
    "\n",
    "even avg.bounce rate is decreasing as time spent increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Administrative_Duration'],df_new['Revenue'],values=df_new['ExitRates'],aggfunc='mean')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see avg.Exit rate when revenue true is very less then when it is false\n",
    "\n",
    "even avg.Exit rate is decreasing as time spent increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Administrative_Duration'],df_new['Revenue'],values=df_new['PageValues'],aggfunc='mean')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see pagevalue is high at duration 0 it is because of other pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informational_Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Informational_Duration'],df_new['Revenue'],values=df_new['BounceRates'],aggfunc='mean')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg.bounce rate when revenue is true is very less than when revenue is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Informational_Duration'],df_new['Revenue'],values=df_new['ExitRates'],aggfunc='mean')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg.exit rate when revenue is true is very less than when revenue is true\n",
    "\n",
    "even avg.Exit rate is decreasing as time spent increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Informational_Duration'],df_new['Revenue'],values=df_new['PageValues'],aggfunc='mean')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see high pagevalue for duration 0 it is because of other pages when this is 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProductRelated_Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['ProductRelated_Duration'],df_new['Revenue'],values=df_new['BounceRates'],aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see avg.bounce rate when revenue true is very less then when it is false\n",
    "\n",
    "even avg.bounce rate is decreasing as time spent increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['ProductRelated_Duration'],df_new['Revenue'],values=df_new['ExitRates'],aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see avg.exit rate when revenue true is very less then when it is false\n",
    "\n",
    "even avg.exit rate is decreasing as time spent increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['ProductRelated_Duration'],df_new['Revenue'],values=df_new['PageValues'],aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page values are very high for product related than other  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can say product related is mostly preffered over other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of sessions for each duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_new['Administrative_Duration'], hue=df['Revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_new['Informational_Duration'], hue=df['Revenue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_new['ProductRelated_Duration'], hue=df['Revenue'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see when sessions are product related then most of the revenue is generated \n",
    "\n",
    "most the time is spent on product related.\n",
    "\n",
    "informational pages are mostly not interested "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region \n",
    "\n",
    "### region n duration vs page value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Region'],df_new['Administrative_Duration'],values=df_new['PageValues'],aggfunc='mean')\n",
    "ct.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Region'],df_new['Informational_Duration'],values=df_new['PageValues'],aggfunc='mean')\n",
    "ct.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Region'],df_new['ProductRelated_Duration'],values=df_new['PageValues'],aggfunc='mean')\n",
    "ct.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### region n duration vs bounce rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Region'],df_new['Administrative_Duration'],values=df_new['BounceRates'],aggfunc='mean')\n",
    "ct.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Region'],df_new['Informational_Duration'],values=df_new['BounceRates'],aggfunc='mean')\n",
    "ct.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=pd.crosstab(df_new['Region'],df_new['ProductRelated_Duration'],values=df_new['BounceRates'],aggfunc='mean')\n",
    "ct.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.get_dummies(data=df_new,columns=['Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend','Revenue'],drop_first=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={'Revenue_True':'Revenue'},inplace=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=df1.copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_p.drop('Revenue',axis=1)\n",
    "y=df_p['Revenue']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression\n",
    "\n",
    "Logistic regression is a type of regression analysis in statistics used for prediction of outcome of a categorical dependent variable from a set of predictor or independent variables. In logistic regression the dependent variable is always binary. Logistic regression is mainly used to for prediction and also calculating the probability of success. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred=logreg.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_reference = pd.DataFrame(np.array([\"TP\",\"FP\",\"FN\",\"TN\"]).reshape(2,2), columns=['Predicted:0','Predicted:1'], index=['Actual:0','Actual:1'])\n",
    "print(cm_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=cm[0,0]\n",
    "TN=cm[1,1]\n",
    "FN=cm[1,0]\n",
    "FP=cm[0,1]\n",
    "sensitivity=TP/float(TP+FN)\n",
    "specificity=TN/float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Negatives :\",TN)\n",
    "print(\"True Positives :\",TP)\n",
    "print(\"False Negative :\",FN,\" (Type II error)\")\n",
    "print(\"False Positives :\",FP,\" (Type I error)\")\n",
    "print(\"correctly predicted :\",TP+TN)\n",
    "print(\"miss-classified :\",FN+FP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The acuuracy of the model = TP+TN / (TP+TN+FP+FN) = ',(TP+TN)/float(TP+TN+FP+FN),'\\n\\n',\n",
    "\n",
    "'The Miss-classification = 1-Accuracy = ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP / (TP+FN) = ',TP/float(TP+FN),'\\n\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN / (TN+FP) = ',TN/float(TN+FP),'\\n\\n',\n",
    "\n",
    "'Positive Predictive value = TP / (TP+FP) = ',TP/float(TP+FP),'\\n\\n',\n",
    "\n",
    "'Negative predictive Value = TN / (TN+FN) = ',TN/float(TN+FN),'\\n\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity / (1-Specificity) = ',sensitivity/(1-specificity),'\\n\\n',\n",
    "\n",
    "'Negative likelihood Ratio = (1-Sensitivity) / Specificity = ',(1-sensitivity)/specificity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above statistics it is clear that the model is highly sensitive than specific. The positive values are predicted more accurately than the negative i.e, it is predicting people who will not make purchase more accurately than who make purchase.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color= Blue>Predicted probabilities of  0 (will not make purchase) and 1 ( purchase)  for the test data with a default classification threshold of 0.5 <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob=logreg.predict_proba(x_test)[:,:]\n",
    "y_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of no purchase (0)','Prob of purchase (1)'])\n",
    "y_pred_prob_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common way to visualize the trade-offs of different thresholds is by using an ROC curve, a plot of the true positive rate ( true positives/ total  positives) versus the false positive rate ( false positives /total  negatives) for all possible choices of thresholds.\n",
    "- A model with good classification accuracy should have significantly more true positives than false positives at all thresholds. \n",
    "- The optimum position for roc curve is towards the top left corner where the specificity and sensitivity are at optimum levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under The Curve (AUC)\n",
    "\n",
    "- The area under the ROC curve quantifies model classification accuracy ; the higher the area, the greater the disparity between true and false positives, and the stronger the model in classifying members of the training dataset.\n",
    "- An area of 0.5 corresponds to a model that performs no better than random classification and a good classifier stays as far away from that  as possible. An area of 1 is ideal. \n",
    "- The closer the AUC to 1 the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results matrix\n",
    "df_results = pd.DataFrame(columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itereation results\n",
    "description = \"Base logit model\"\n",
    "misclassifications = FP + FN\n",
    "type1 = FP\n",
    "type2 = FN\n",
    "precision = round(precision_score(y_test,y_pred),2)\n",
    "recall = round(recall_score(y_test,y_pred),2)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),2)\n",
    "f1 = round(f1_score(y_test,y_pred),2)\n",
    "auc = round(roc_auc_score(y_test,y_pred),2)\n",
    "\n",
    "df_results = pd.concat([df_results,\n",
    "                        pd.DataFrame(np.array([description,\n",
    "                                     misclassifications,\n",
    "                                     type1,\n",
    "                                     type2,\n",
    "                                     precision,\n",
    "                                     recall,\n",
    "                                     accuracy,\n",
    "                                     f1,\n",
    "                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n",
    "                                  ], axis=0)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshold\n",
    "- Since the model is predicting people who dont purchase too many type II errors is not advisable. \n",
    "- A False Negative ( ignoring the probability of purchase when there actualy is one) is more dangerous than a False Positive in this case.\n",
    "- Hence inorder to increase the sensitivity,  threshold can be lowered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(1,5):\n",
    "  cm2 = 0\n",
    "  y_pred_prob_yes = logreg.predict_proba(x_test)\n",
    "  y_pred2 = np.where(y_pred_prob_yes[:, 1] > i/10, 1, 0)\n",
    "  cm2 = confusion_matrix(y_test, y_pred2)\n",
    "  print('With', i/10, 'threshold the Confusion Matrix is', '\\n', cm2, '\\n',\n",
    "        'with', cm2[0, 0] + cm2[1, 1], 'correct predictions and', cm2[1, 0], 'Type II errors (False Negatives)', '\\n\\n',\n",
    "        'Sensitivity:', cm2[1, 1] / (float(cm2[1, 1] + cm2[1, 0])), 'Specificity:', cm2[0, 0] / (float(cm2[0, 0] + cm2[0, 1])), '\\n\\n\\n')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Curva de ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for online shoppers classifier')\n",
    "plt.xlabel('False positive rate (1-Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1)Here,we can see that even decreasing threshold it is causing type 1 error.\n",
    "\n",
    "## 2)we can use the default threshold 0.5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will use kfold cv to decide which model performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT=DecisionTreeClassifier()\n",
    "KNN=KNeighborsClassifier(n_neighbors=9,weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [] \n",
    "models.append(('DT',DT))\n",
    "models.append(('KNN',KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=3,shuffle=True,random_state=0)\n",
    "    cv_results = model_selection.cross_val_score(model,X,y,cv=kfold,scoring='f1_weighted')\n",
    "    results.append(np.sqrt(np.abs(cv_results)))\n",
    "    names.append(name)\n",
    "    print(\"%s: %f (%f)\" % (name, 1-np.mean(cv_results),np.std(cv_results,ddof=1)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos têm valores valores relativamente baixos, mas o KNN tem um valor melhor. Contudo vamos contruir ambos os modelos e compara-los\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=KNeighborsClassifier().fit(x_train,y_train)\n",
    "y_pred_rf=rf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred_rf)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "\n",
    "TP=cm[0,0]\n",
    "TN=cm[1,1]\n",
    "FN=cm[1,0]\n",
    "FP=cm[0,1]\n",
    "sensitivity=TP/float(TP+FN)\n",
    "specificity=TN/float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itereation results\n",
    "description = \"KNeighbors Classifier\"  #change the name of models\n",
    "misclassifications = FP + FN\n",
    "type1 = FP\n",
    "type2 = FN\n",
    "precision = round(precision_score(y_test,y_pred_rf),2)\n",
    "recall = round(recall_score(y_test,y_pred_rf),2)\n",
    "accuracy = round(accuracy_score(y_test,y_pred_rf),2)\n",
    "f1 = round(f1_score(y_test,y_pred_rf),2)\n",
    "auc = round(roc_auc_score(y_test,y_pred_rf),2)\n",
    "\n",
    "df_results = pd.concat([df_results,\n",
    "                        pd.DataFrame(np.array([description,\n",
    "                                     misclassifications,\n",
    "                                     type1,\n",
    "                                     type2,\n",
    "                                     precision,\n",
    "                                     recall,\n",
    "                                     accuracy,\n",
    "                                     f1,\n",
    "                                     auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n",
    "                                  ], axis=0)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=DecisionTreeClassifier().fit(x_train,y_train)\n",
    "y_pred_ada=ada.predict(x_test)\n",
    "print(classification_report(y_test,y_pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred_ada)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "\n",
    "TP=cm[0,0]\n",
    "TN=cm[1,1]\n",
    "FN=cm[1,0]\n",
    "FP=cm[0,1]\n",
    "sensitivity=TP/float(TP+FN)\n",
    "specificity=TN/float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados\n",
    "description = \"Decision Tree Classifier\"  #change the name of models\n",
    "misclassifications = FP + FN\n",
    "type1 = FP\n",
    "type2 = FN\n",
    "precision = round(precision_score(y_test,y_pred_ada),2)\n",
    "recall = round(recall_score(y_test,y_pred_ada),2)\n",
    "accuracy = round(accuracy_score(y_test,y_pred_ada),2)\n",
    "f1 = round(f1_score(y_test,y_pred_ada),2)\n",
    "auc = round(roc_auc_score(y_test,y_pred_ada),2)\n",
    "\n",
    "df_results = pd.concat([df_results,\n",
    "pd.DataFrame(np.array([description,\n",
    "   misclassifications,\n",
    "   type1,\n",
    "   type2,\n",
    "   precision,\n",
    "   recall,\n",
    "   accuracy,\n",
    "   f1,\n",
    "   auc]).reshape(1,-1), columns=['Description','Misclassifications','Type I errors','Type II errors','Precision','Recall','Accuracy','F1-score','ROC AUC'])\n",
    "], axis=0)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar o dataset\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv\")\n",
    "\n",
    "# Identificar outliers na coluna \"Revenue\"\n",
    "coluna = 'Revenue'\n",
    "\n",
    "# Visualizar a distribuição dos valores usando um box plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(data[coluna].astype(int), vert=False)\n",
    "plt.title(f\"Box Plot de {coluna}\")\n",
    "plt.xlabel(coluna)\n",
    "plt.show()\n",
    "\n",
    "# Definir um limite para identificar outliers (por exemplo, 3 desvios-padrão)\n",
    "limite_outliers = data[coluna].mean() + 3 * data[coluna].std()\n",
    "\n",
    "# Identificar os outliers com base no limite definido\n",
    "outliers = data[data[coluna].astype(int) > limite_outliers]\n",
    "\n",
    "# Remover os outliers do dataset\n",
    "data_sem_outliers = data[data[coluna].astype(int) <= limite_outliers]\n",
    "\n",
    "# Exibir informações sobre os outliers identificados\n",
    "print(\"Outliers:\")\n",
    "print(outliers)\n",
    "\n",
    "# Exibir informações sobre o dataset sem outliers\n",
    "print(\"\\nDataset sem outliers:\")\n",
    "print(data_sem_outliers.head())\n",
    "\n",
    "# Visualizar a distribuição dos valores sem outliers usando um histograma\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(data_sem_outliers[coluna].astype(int), bins=2)\n",
    "plt.title(f\"Histograma de {coluna} sem outliers\")\n",
    "plt.xlabel(coluna)\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
